{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b76fc59",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    " This is a 2 hour workshop exploring the fundamentals of monte carlo methods as used in particle physics and astronomy.\n",
    " The workshop is designed to be run in a Jupyter notebook\n",
    " \n",
    " By the end of this workshop you will have explored the core concepts of:\n",
    " 1. Probability distributions\n",
    " 2. Samples\n",
    " 3. Sampling\n",
    " 4. Monte Carlo integration\n",
    " \n",
    " and the fundamental concepts of the core tools we use to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88386d7f",
   "metadata": {},
   "source": [
    "## 0. Setup\n",
    " I will presume that none of these commands are unfamiliar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78481de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffaa8e0",
   "metadata": {},
   "source": [
    "## 1. Probability distributions\n",
    " \n",
    "\n",
    " __Recap:__ a probability distribution $P$ on a variable $x$ is defined such that \n",
    "  $$P(a<x<b) = \\int_a^b P(x) dx,$$\n",
    " or equivalently that $P(x)dx$ represents the probability that $x$ lies in the interval $[x,x+dx]$.\n",
    "\n",
    " The [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) module is full of a whole variety of distributions, which forms the basis of a lot of other software.\n",
    " Let's create an example with a [von Mises distribution](https://en.wikipedia.org/wiki/Von_Mises_distribution) (an example you may not have seen before), rather than starting with a boring normal distribution\n",
    " $$P(x) = \\frac{e^{\\kappa \\cos(x-\\mu)}}{2\\pi I_0(\\kappa)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3235e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa = 1\n",
    "mu = np.pi\n",
    "dist = scipy.stats.vonmises(kappa, mu)\n",
    "x = np.linspace(0, 2*np.pi,1000)\n",
    "p = dist.pdf(x)\n",
    "plt.plot(x, p);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13170690",
   "metadata": {},
   "source": [
    "__Exercise 1:__ produce a plot showing different choices of `kappa` and `mu`?\n",
    " - __Question 1.1:__ What happens to the plot if you set `kappa` very large?\n",
    " - __Question 1.2:__ Try out a few other distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa21de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 1\n",
    "# Write your answer here into this code block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a34aaa",
   "metadata": {},
   "source": [
    "uncomment and execute the below to see the solution (only after you've had a go yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e091a",
   "metadata": {},
   "source": [
    "### Two dimensional distributions\n",
    " These concepts are extended relatively straightforwardly to two-dimensional distributions $P(x,y)\n",
    "\n",
    " For a two-dimensional [von mises Fisher distribution](https://en.wikipedia.org/wiki/Von_Mises%E2%80%93Fisher_distribution) we can plot it as a contour plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters of von-mises in terms of concentration kappa and mean direction (phi0, theta0)\n",
    "\n",
    "def vmf_dist(kappa, phi, theta):\n",
    "    mu = np.array([np.cos(phi)*np.sin(theta),\n",
    "                   np.sin(phi)*np.sin(theta),\n",
    "                   np.cos(theta)])\n",
    "    return scipy.stats.vonmises_fisher(mu, kappa)\n",
    "\n",
    "kappa = 1\n",
    "phi0 = np.pi\n",
    "theta0 = np.pi/2\n",
    "dist = vmf_dist(kappa, phi0, theta0)\n",
    "\n",
    "# Compute a meshgrid \n",
    "phi, theta = np.linspace(0, 2*np.pi, 100), np.arccos(np.linspace(1, -1, 100))\n",
    "phi, theta = np.meshgrid(phi, theta)\n",
    "x, y, z  = np.sin(theta) * np.cos(phi), np.sin(theta) * np.sin(phi), np.cos(theta)\n",
    "\n",
    "# Compute the pdf on the meshgrid with numpy broadcasting\n",
    "x = np.stack([x, y, z], axis=2)\n",
    "pdf = dist.pdf(x)\n",
    "plt.contourf(phi, theta, pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a34e0",
   "metadata": {},
   "source": [
    "__Exercise 2:__ Copy and paste the above code and adjust it to get an understanding of the effect of  `kappa` and `mu`?\n",
    " - __Question 2.1:__ What do the colours represent?\n",
    " - __Question 2.2:__ What is the relevance of the `arccos` in the above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer 2\n",
    "# Write your answer here into this code block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8292e9b5",
   "metadata": {},
   "source": [
    "The colours above correspond to a matplotlib default colour scheme mapped to the values of the probability density.\n",
    "\n",
    " It is more usual to plot 'one sigma' and 'two sigma' contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51aa4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = vmf_dist(10, np.pi, np.pi/4).pdf(x)\n",
    "pdf = pdf.reshape(-1)\n",
    "i = np.argsort(pdf)\n",
    "cdf = pdf[i].cumsum()\n",
    "cdf /= cdf[-1]\n",
    "sigma = np.empty_like(pdf)\n",
    "sigma[i] = np.sqrt(2) * scipy.special.erfinv(1-cdf)\n",
    "sigma = sigma.reshape(phi.shape)\n",
    "plt.contourf(phi, theta, sigma, levels=[0, 1, 2], colors=['black', 'gray'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057f3c2f",
   "metadata": {},
   "source": [
    "- __Question 3.1:__ What exactly do \"one sigma\" and \"two sigma\" contours mean, and how does the above code compute them?\n",
    " - __Question 3.2:__ How should one choose the colours for the contours? (have a look at the [anesthetic](https://github.com/handley-lab/anesthetic#another-posterior-plotting-tool) README for one option for doing this.\n",
    "\n",
    " __Beware:__ [corner.py](https://corner.readthedocs.io/en/latest/pages/sigmas/) uses a different definition of sigma."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f87ff3",
   "metadata": {},
   "source": [
    "### Higher dimensional distributions\n",
    " In most cases we have far more than two variables, which makes plotting the full distribution difficult.\n",
    " \n",
    " The best we can do visually is to plot the _marginal distributions_, i.e. the distributions with each other variable integrated out. Since 1D and 2D distributions are plottable, we typically do this by considering the 1D marginals individually:\n",
    " $$ P(x_i) = \\int P(x) \\prod\\limits_{k\\ne i} dx_k$$\n",
    " and pairwise marginals\n",
    " $$ P(x_i, x_j) = \\int P(x) \\prod\\limits_{k\\ne i, k\\ne j} dx_k$$\n",
    " and arrange these into a \"corner\" or \"triangle\" plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef9b0e6",
   "metadata": {},
   "source": [
    "This example takes a few seconds to generate and plot -- don't worry about the details, we will cover them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anesthetic.examples.perfect_ns import planck_gaussian\n",
    "params = ['omegabh2', 'omegach2', 'theta', 'tau', 'logA', 'ns']\n",
    "s = planck_gaussian()[params].plot_2d(kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac2842b",
   "metadata": {},
   "source": [
    "The next section builds up to how we go about producing these plots in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba74e4",
   "metadata": {},
   "source": [
    "## 1. Why do sampling\n",
    " The core concept in numerical inference is that of *samples*.\n",
    "\n",
    " Given some a-priori unknown probability distribution $P(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e6d7c",
   "metadata": {},
   "source": [
    "Some example code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fe9098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "samples = np.random.rand(5)\n",
    "print(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7875618",
   "metadata": {},
   "source": [
    "uncomment the below to see the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/1.py"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
